{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW - Continuous Bag of Words - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das variantes do word2vec é o modelo CBOW (Continuous bag of words). <br>\n",
    "A ideia desse modelo é obter os embeddings implicitamente por meio de uma tarefa que parece não fazer muito sentido a priori. Vamos treinar uma rede neural rasa que, a partir de *palavras de contexto* vai tentar prever a palavra central. <br>\n",
    "As palavras de contexto são definidas como as que estão numa janela N ao redor da palavra central. Por exemplo, com a frase `eu quero comer morango com batata`, considerando um N=2 e a palavra central `morango` temos as palavras de contexto `[quero, comer, com, batata]`. A rede deve receber as palavras de contexto e prever a palavra central.\n",
    "Essa tarefa é dita \"auto supervisionada\" porque os rótulos não são dados por um ser humano, mas são palavras do proprio texto. <br>\n",
    "Bom, mas nós não alimentamos as strings puras no modelo. A rede é alimentada com uma média dos vetores *one hot* das palavras de contexto e deve prever o *one hot* da palavra central. \n",
    "Vetores *one hot* são vetores populados apenas com zeros, com exceção de um elemento que é o 1. Exemplificando: vamos supor que temos um vocabulário de 3 palavras: 'azul', 'rosa' e 'preto', nesse caso, os *one hot* seriam, [1,0,0], [0,1,0] e [0,0,1] para azul', 'rosa' e 'preto', respectivamente. Nesse caso, a média dos três vetores *one-hot* seria [$\\frac{1}{3}$, $\\frac{1}{3}$, $\\frac{1}{3}$]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.geeksforgeeks.org/wp-content/uploads/cbow-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portanto, ao receber um texto, devemos prepará-lo para que tenhamos features e rótulos para alimentar o modelo. Para isso, devemos, primeiro extrair os períodos do texto, tokenizá-los e tirar caractéres que não sejam de nosso interesse, como pontuação e números. <br>\n",
    "Uma vez feito isso, conseguimos definir nosso vocabulário e os *one-hots* correspondentes a cada palavra. <br>\n",
    "Depois, \"escaneamos\" cada sentença dentro da janela prevista, preparando vetores X, com as médias dos *one hot* das palavras de contexto, associados a um Y, o *one hot* da palavra central. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"biblia-em-txt.txt\", \"r\")\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BÍBLIA SAGRADA\\nTradução: João Ferreira de Almeida\\nEdição Revista e Corrigida\\n \\t\\nANTIGO TESTAMENTO\\n\\nGÊNESIS\\n GÊNESIS 1\\n1 No princípio criou Deus os céus e a terra.\\n2 A terra era sem forma e vazia; e havia trevas sobre a face do abismo, mas o Espírito de Deus pairava sobre a face das águas.\\n3 Disse Deus: haja luz. E houve luz.\\n4 Viu Deus que a luz era boa; e fez separação entre a luz e as trevas.\\n5 E Deus chamou à luz dia, e às trevas noite. E foi a tarde e a manhã, o dia primeiro.\\n6 E disse Deus:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentece(sent):\n",
    "    tokenized = word_tokenize(sent)\n",
    "    tokenized = [token.lower() for token in tokenized if token.isalpha()]\n",
    "    return tokenized\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tokenized_senteces = [tokenize_sentece(sent) for sent in sentences]\n",
    "    return tokenized_senteces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = tokenize_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os tokens ficam assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bíblia',\n",
       "  'sagrada',\n",
       "  'tradução',\n",
       "  'joão',\n",
       "  'ferreira',\n",
       "  'de',\n",
       "  'almeida',\n",
       "  'edição',\n",
       "  'revista',\n",
       "  'e',\n",
       "  'corrigida',\n",
       "  'antigo',\n",
       "  'testamento',\n",
       "  'gênesis',\n",
       "  'gênesis',\n",
       "  'no',\n",
       "  'princípio',\n",
       "  'criou',\n",
       "  'deus',\n",
       "  'os',\n",
       "  'céus',\n",
       "  'e',\n",
       "  'a',\n",
       "  'terra'],\n",
       " ['a',\n",
       "  'terra',\n",
       "  'era',\n",
       "  'sem',\n",
       "  'forma',\n",
       "  'e',\n",
       "  'vazia',\n",
       "  'e',\n",
       "  'havia',\n",
       "  'trevas',\n",
       "  'sobre',\n",
       "  'a',\n",
       "  'face',\n",
       "  'do',\n",
       "  'abismo',\n",
       "  'mas',\n",
       "  'o',\n",
       "  'espírito',\n",
       "  'de',\n",
       "  'deus',\n",
       "  'pairava',\n",
       "  'sobre',\n",
       "  'a',\n",
       "  'face',\n",
       "  'das',\n",
       "  'águas']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(sent_tokenized):\n",
    "    i = 0\n",
    "    vocab = {}\n",
    "    for sent in sent_tokenized:\n",
    "        for token in sent:\n",
    "            if token in vocab:\n",
    "                continue\n",
    "            else:\n",
    "                vocab[token] = i\n",
    "                i+=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = make_vocab(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(sent_tokens, vocab, V, C):\n",
    "    windows_and_target = []\n",
    "    for sentence in sent_tokens:\n",
    "        for x, y in get_windows(sentence,C):\n",
    "            windows_and_target.append((x,y))\n",
    "        \n",
    "    return windows_and_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = make_dataset(sent_tokens, vocab, V, C=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos o dataset na seguinte estrutura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['do', 'senhor', 'deus', 'para'], 'nosso')\n",
      "(['agague', 'veio', 'ele', 'animosamente'], 'a')\n",
      "(['ó', 'chefes', 'jacó', 'e'], 'de')\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(low=0,high=len(dataset), size=3)\n",
    "for i in idx:\n",
    "    print(dataset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/P9jnI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos uma rede neural rasa, o input $X$ é a média dos vetores *one hot* do contexto. A rede é descrita pelas seguintes equações: <br>\n",
    "$$Z_1 = W_1X + b_1$$\n",
    "$$h = ReLU(Z_1)$$\n",
    "$$Z_2 = W_1h + b_2$$\n",
    "$$ŷ = softmax(Z_2)$$\n",
    "<br>\n",
    "Em que a função softmax pode ser definida:\n",
    "$$softmax(x_i) = \\frac{e^{x_i}}{\\sum_{k=1}^N e^{x_k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"height:400px;\" src = \"https://mermaid.ink/img/eyJjb2RlIjoiZ3JhcGggVERcbiAgQVtJbnB1dF0gLS0-IE1NMSgqKVxuICBXMSAtLT4gTU0xXG4gIE1NMSAtLT4gWjFcbiAgYjEgLS0-IFoxXG4gIFoxIC0tPiB8UmVMVXxoXG4gIGggLS0-IE1NMigqKVxuICBXMiAtLT4gTU0yKCopXG4gIE1NMiAtLT4gWjJcbiAgYjIgLS0-IFoyXG4gIFoyIC0tPiB8c29mdG1heHzFt1xuXG4iLCJtZXJtYWlkIjp7InRoZW1lIjoiZGVmYXVsdCIsInRoZW1lVmFyaWFibGVzIjp7ImJhY2tncm91bmQiOiJ3aGl0ZSIsInByaW1hcnlDb2xvciI6IiNFQ0VDRkYiLCJzZWNvbmRhcnlDb2xvciI6IiNmZmZmZGUiLCJ0ZXJ0aWFyeUNvbG9yIjoiaHNsKDgwLCAxMDAlLCA5Ni4yNzQ1MDk4MDM5JSkiLCJwcmltYXJ5Qm9yZGVyQ29sb3IiOiJoc2woMjQwLCA2MCUsIDg2LjI3NDUwOTgwMzklKSIsInNlY29uZGFyeUJvcmRlckNvbG9yIjoiaHNsKDYwLCA2MCUsIDgzLjUyOTQxMTc2NDclKSIsInRlcnRpYXJ5Qm9yZGVyQ29sb3IiOiJoc2woODAsIDYwJSwgODYuMjc0NTA5ODAzOSUpIiwicHJpbWFyeVRleHRDb2xvciI6IiMxMzEzMDAiLCJzZWNvbmRhcnlUZXh0Q29sb3IiOiIjMDAwMDIxIiwidGVydGlhcnlUZXh0Q29sb3IiOiJyZ2IoOS41MDAwMDAwMDAxLCA5LjUwMDAwMDAwMDEsIDkuNTAwMDAwMDAwMSkiLCJsaW5lQ29sb3IiOiIjMzMzMzMzIiwidGV4dENvbG9yIjoiIzMzMyIsIm1haW5Ca2ciOiIjRUNFQ0ZGIiwic2Vjb25kQmtnIjoiI2ZmZmZkZSIsImJvcmRlcjEiOiIjOTM3MERCIiwiYm9yZGVyMiI6IiNhYWFhMzMiLCJhcnJvd2hlYWRDb2xvciI6IiMzMzMzMzMiLCJmb250RmFtaWx5IjoiXCJ0cmVidWNoZXQgbXNcIiwgdmVyZGFuYSwgYXJpYWwiLCJmb250U2l6ZSI6IjE2cHgiLCJsYWJlbEJhY2tncm91bmQiOiIjZThlOGU4Iiwibm9kZUJrZyI6IiNFQ0VDRkYiLCJub2RlQm9yZGVyIjoiIzkzNzBEQiIsImNsdXN0ZXJCa2ciOiIjZmZmZmRlIiwiY2x1c3RlckJvcmRlciI6IiNhYWFhMzMiLCJkZWZhdWx0TGlua0NvbG9yIjoiIzMzMzMzMyIsInRpdGxlQ29sb3IiOiIjMzMzIiwiZWRnZUxhYmVsQmFja2dyb3VuZCI6IiNlOGU4ZTgiLCJhY3RvckJvcmRlciI6ImhzbCgyNTkuNjI2MTY4MjI0MywgNTkuNzc2NTM2MzEyOCUsIDg3LjkwMTk2MDc4NDMlKSIsImFjdG9yQmtnIjoiI0VDRUNGRiIsImFjdG9yVGV4dENvbG9yIjoiYmxhY2siLCJhY3RvckxpbmVDb2xvciI6ImdyZXkiLCJzaWduYWxDb2xvciI6IiMzMzMiLCJzaWduYWxUZXh0Q29sb3IiOiIjMzMzIiwibGFiZWxCb3hCa2dDb2xvciI6IiNFQ0VDRkYiLCJsYWJlbEJveEJvcmRlckNvbG9yIjoiaHNsKDI1OS42MjYxNjgyMjQzLCA1OS43NzY1MzYzMTI4JSwgODcuOTAxOTYwNzg0MyUpIiwibGFiZWxUZXh0Q29sb3IiOiJibGFjayIsImxvb3BUZXh0Q29sb3IiOiJibGFjayIsIm5vdGVCb3JkZXJDb2xvciI6IiNhYWFhMzMiLCJub3RlQmtnQ29sb3IiOiIjZmZmNWFkIiwibm90ZVRleHRDb2xvciI6ImJsYWNrIiwiYWN0aXZhdGlvbkJvcmRlckNvbG9yIjoiIzY2NiIsImFjdGl2YXRpb25Ca2dDb2xvciI6IiNmNGY0ZjQiLCJzZXF1ZW5jZU51bWJlckNvbG9yIjoid2hpdGUiLCJzZWN0aW9uQmtnQ29sb3IiOiJyZ2JhKDEwMiwgMTAyLCAyNTUsIDAuNDkpIiwiYWx0U2VjdGlvbkJrZ0NvbG9yIjoid2hpdGUiLCJzZWN0aW9uQmtnQ29sb3IyIjoiI2ZmZjQwMCIsInRhc2tCb3JkZXJDb2xvciI6IiM1MzRmYmMiLCJ0YXNrQmtnQ29sb3IiOiIjOGE5MGRkIiwidGFza1RleHRMaWdodENvbG9yIjoid2hpdGUiLCJ0YXNrVGV4dENvbG9yIjoid2hpdGUiLCJ0YXNrVGV4dERhcmtDb2xvciI6ImJsYWNrIiwidGFza1RleHRPdXRzaWRlQ29sb3IiOiJibGFjayIsInRhc2tUZXh0Q2xpY2thYmxlQ29sb3IiOiIjMDAzMTYzIiwiYWN0aXZlVGFza0JvcmRlckNvbG9yIjoiIzUzNGZiYyIsImFjdGl2ZVRhc2tCa2dDb2xvciI6IiNiZmM3ZmYiLCJncmlkQ29sb3IiOiJsaWdodGdyZXkiLCJkb25lVGFza0JrZ0NvbG9yIjoibGlnaHRncmV5IiwiZG9uZVRhc2tCb3JkZXJDb2xvciI6ImdyZXkiLCJjcml0Qm9yZGVyQ29sb3IiOiIjZmY4ODg4IiwiY3JpdEJrZ0NvbG9yIjoicmVkIiwidG9kYXlMaW5lQ29sb3IiOiJyZWQiLCJsYWJlbENvbG9yIjoiYmxhY2siLCJlcnJvckJrZ0NvbG9yIjoiIzU1MjIyMiIsImVycm9yVGV4dENvbG9yIjoiIzU1MjIyMiIsImNsYXNzVGV4dCI6IiMxMzEzMDAiLCJmaWxsVHlwZTAiOiIjRUNFQ0ZGIiwiZmlsbFR5cGUxIjoiI2ZmZmZkZSIsImZpbGxUeXBlMiI6ImhzbCgzMDQsIDEwMCUsIDk2LjI3NDUwOTgwMzklKSIsImZpbGxUeXBlMyI6ImhzbCgxMjQsIDEwMCUsIDkzLjUyOTQxMTc2NDclKSIsImZpbGxUeXBlNCI6ImhzbCgxNzYsIDEwMCUsIDk2LjI3NDUwOTgwMzklKSIsImZpbGxUeXBlNSI6ImhzbCgtNCwgMTAwJSwgOTMuNTI5NDExNzY0NyUpIiwiZmlsbFR5cGU2IjoiaHNsKDgsIDEwMCUsIDk2LjI3NDUwOTgwMzklKSIsImZpbGxUeXBlNyI6ImhzbCgxODgsIDEwMCUsIDkzLjUyOTQxMTc2NDclKSJ9fSwidXBkYXRlRWRpdG9yIjpmYWxzZX0/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nossa função de custo é a entropia cruzada, que é definida da seguinte forma:\n",
    "$$J = - \\sum_{k=1}^V y_k \\ln\\hat{y}_k$$\n",
    "Em que V denota o número de palavras no vocabulário. <br>\n",
    "\n",
    "Perceba que essa função de custo faz sentido para o nosso problema, pois quando $y_k$ é igual a zero, independetemente do valor de $\\hat{y}_k$, a multiplicação será zero. O resultado do somatório será apenas o $-\\ln \\hat{y}_k$ para o $\\hat{y}_k$ correspondente ao $k$ da classe correta.\n",
    "\n",
    " - Se $\\hat{y}_k$ se aproxima de 1 (o que nós queremos, pois isso representa a probabilidade prevista para o modelo para a classe correta) $J$ se aproxima de 0; \n",
    " - Se $\\hat{y}_k$ se aproximar de 0, $J$ explode para o infinito (já que $\\ln x$ vai para - infinito quando x se aproxima de 0)<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos minimizar a nossa função de custo. Para isso, vamos aplicar o gradiente descendente nos nossos parâmetros aprendidos $W_1$, $W_2$, $b_1$ e $b_2$. Isso será feito utilizando backpropagation. \n",
    "As expressões das derivadas não devem ser difíceis de se obter com um conhecimento básico de cálculo diferencial, com exceção da derivação $\\frac{\\partial J}{\\partial Z_2}$, ela pode ser entendida [neste vídeo](https://www.youtube.com/watch?v=5-rVLSc2XdE). <br>\n",
    "\n",
    "As expressões são (para um batch com $m$ elementos):\n",
    "$$\\frac{\\partial J_b}{\\partial W_2} = \\frac{1}{m}(\\hat{y} - y)h^t$$\n",
    "$$\\frac{\\partial J_b}{\\partial b_2} = \\sum_{linhas}\\frac{1}{m}(\\hat{y} - y)$$\n",
    "$$\\frac{\\partial J_b}{\\partial W_1} = \\frac{1}{m}ReLU(W_2^T(\\hat{y} - y))X^T$$\n",
    "$$\\frac{\\partial J_b}{\\partial b_1} = \\sum_{linhas}\\frac{1}{m}ReLU(W_2^T(\\hat{y} - y))$$\n",
    "<br>\n",
    "E, como praxe do gradiente descendente, os parâmtros serão atualizados ($\\alpha$ é a taxa de aprendizado):\n",
    "$$W_2 := W_2 - \\alpha \\cdot \\frac{\\partial J_b}{\\partial W_2}$$\n",
    "$$b_2 := b_2 - \\alpha \\cdot \\frac{\\partial J_b}{\\partial b_2}$$\n",
    "$$W_1 := W_1 - \\alpha \\cdot \\frac{\\partial J_b}{\\partial W_1}$$\n",
    "$$b_1 := b_1 - \\alpha \\cdot \\frac{\\partial J_b}{\\partial b_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação com Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_one_hot_vector(word, vocab, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[vocab[word]] = 1\n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(raw_batch, vocab, V, batch_size):\n",
    "    batch_x = []\n",
    "    batch_y = []\n",
    "    for x, y in raw_batch:\n",
    "        while len(batch_x) < batch_size:\n",
    "            batch_x.append(np.mean([word_to_one_hot_vector(w, vocab, V) for w in x], axis=0))\n",
    "            batch_y.append(word_to_one_hot_vector(y, vocab, V))\n",
    "        else:\n",
    "            return np.array(batch_x).T, np.array(batch_y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(V,emb_size):\n",
    "    W1 = np.random.rand(emb_size,V)\n",
    "    W2 = np.random.rand(V,emb_size)\n",
    "    b1 = np.random.rand(emb_size,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z, axis=0) \n",
    "    soft = e_z / sum_e_z\n",
    "    return soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(z,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward(x, W1, W2, b1, b2):\n",
    "    z1 = W1@x + b1\n",
    "    h = relu(z1)\n",
    "    z2 = W2@h + b2\n",
    "    yhat = softmax(z2)\n",
    "    return yhat, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size):\n",
    "    logprobs = y*np.log(yhat)   \n",
    "    cost = - (1/batch_size) * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    l1 =(1/batch_size)* relu((W2.T@(yhat - y)))\n",
    "    grad_W1 = (1/batch_size)*relu((l1@x.T))\n",
    "    grad_W2 = (1/batch_size)*np.dot((yhat - y),h.T)\n",
    "    grad_b1 = (1/batch_size)*relu(np.sum(l1, axis=1, keepdims=True))\n",
    "    grad_b2 =(1/batch_size)*(np.sum(yhat - y, axis=1, keepdims=True))    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0| epoch 1| cost: 17.602026366344155\n",
      "iteration: 200| epoch 1| cost: 7.267899523665009\n",
      "iteration: 400| epoch 1| cost: 0.36458694997025975\n",
      "iteration: 600| epoch 1| cost: 0.08660287189701525\n",
      "iteration: 800| epoch 1| cost: 0.04702851156348532\n",
      "iteration: 1000| epoch 1| cost: 0.03203897616005174\n",
      "iteration: 1200| epoch 1| cost: 0.024233489473208983\n",
      "iteration: 1400| epoch 1| cost: 0.019463516711696348\n",
      "iteration: 1600| epoch 1| cost: 0.0162523226370466\n",
      "iteration: 1800| epoch 1| cost: 0.013945437874155775\n",
      "iteration: 2000| epoch 1| cost: 0.012209084302997416\n",
      "iteration: 2200| epoch 1| cost: 0.010855451430703164\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "EPOCHS = 1\n",
    "ALPHA = 0.001\n",
    "emb_size = 50\n",
    "\n",
    "\n",
    "W1, W2, b1, b2 = initialize_params(V,emb_size)\n",
    "for i in range(EPOCHS):\n",
    "    for j in range(int(len(dataset)/BATCH_SIZE)):\n",
    "        x,y = get_batch(dataset, vocab, V, BATCH_SIZE)\n",
    "        yhat, h = foward(x, W1, W2, b1, b2)\n",
    "        cost = compute_cost(y, yhat, BATCH_SIZE)\n",
    "        if j%200 == 0:\n",
    "            print(f\"iteration: {j}| epoch {i+1}| cost: {cost}\")\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, BATCH_SIZE)\n",
    "        W1 -= ALPHA*grad_W1\n",
    "        W2-= ALPHA*grad_W2\n",
    "        b1 -= ALPHA*grad_b1\n",
    "        b2 -= ALPHA*grad_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = (W1.T + W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_embeddings = {}\n",
    "for word, i in vocab.items():\n",
    "    encoded_embeddings[word] = list(embeddings[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'bible_embeddings.json', 'w') as file:\n",
    "    file.write(json.dumps(encoded_embeddings)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tweets",
   "language": "python",
   "name": "tweets"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
